{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_face_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Фреймфорк: torchvision.models.detection\n",
        "3. Датасет: WIDER face\n",
        "\n",
        "Я не сог обучить модель. Не понимаю, что делать с переполнением памяти в CUDA, в google colab."
      ],
      "metadata": {
        "id": "gV58YVsMr9N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1ak1rZgRDfxXYIFg7RUx8o8bTLwHntdee\n",
        "!unzip WIDER.zip > /dev/null"
      ],
      "metadata": {
        "id": "nG0HHeZimD2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b268ce10-0959-435e-a9cc-ba2a711d1f7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ak1rZgRDfxXYIFg7RUx8o8bTLwHntdee\n",
            "To: /content/WIDER.zip\n",
            "100% 3.68G/3.68G [00:30<00:00, 120MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.8.2\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7dEQIdmX1f9",
        "outputId": "b9a05f47-1be8-4b45-d1db-3a2b840e8099"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 91740, done.\u001b[K\n",
            "remote: Counting objects: 100% (26871/26871), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2367/2367), done.\u001b[K\n",
            "remote: Total 91740 (delta 24733), reused 26284 (delta 24364), pack-reused 64869\n",
            "Receiving objects: 100% (91740/91740), 181.56 MiB | 27.09 MiB/s, done.\n",
            "Resolving deltas: 100% (77808/77808), done.\n",
            "Note: checking out 'v0.8.2'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 2f40a483d [v0.8.X] .circleci: Add Python 3.9 to CI (#3063)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8gyGjM1cdixk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import abspath, expanduser\n",
        "from typing import Any, Callable, List, Dict, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision.datasets.utils import (\n",
        "    download_file_from_google_drive,\n",
        "    download_and_extract_archive,\n",
        "    extract_archive,\n",
        "    verify_str_arg,\n",
        ")\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "\n",
        "class WiderDataset(VisionDataset):\n",
        "    BASE_FOLDER = \"WIDER\"\n",
        "    FILE_LIST = [\n",
        "        # File ID                             MD5 Hash                            Filename\n",
        "        (\"15hGDLhsx8bLgLcIRD5DhYt5iBxnjNF1M\", \"3fedf70df600953d25982bcd13d91ba2\", \"WIDER_train.zip\"),\n",
        "        (\"1GUCogbp16PMGa39thoMMeWxp7Rp5oM8Q\", \"dfa7d7e790efa35df3788964cf0bbaea\", \"WIDER_val.zip\"),\n",
        "        (\"1HIfDbVEWKmsYKJZm4lchTBDLW5N7dY5T\", \"e5d8f4248ed24c334bbd12f49c29dd40\", \"WIDER_test.zip\"),\n",
        "    ]\n",
        "    ANNOTATIONS_FILE = (\n",
        "        \"http://shuoyang1213.me/WIDERFACE/support/bbx_annotation/wider_face_split.zip\",\n",
        "        \"0e3767bcf0e326556d407bf5bff5d27c\",\n",
        "        \"wider_face_split.zip\",\n",
        "    )\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        split: str = \"train\",\n",
        "        transform: Optional[Callable] = None,\n",
        "        download: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(root=os.path.join(root, self.BASE_FOLDER), transform=transform)\n",
        "        \n",
        "        # check arguments\n",
        "        self.split = verify_str_arg(split, \"split\", (\"train\", \"val\", \"test\"))\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError(\"Dataset not found or corrupted. You can use download=True to download and prepare it\")\n",
        "\n",
        "        self.img_info: List[Dict[str, Union[str, Dict[str, torch.Tensor]]]] = []\n",
        "        if self.split in (\"train\", \"val\"):\n",
        "            self.parse_train_val_annotations_file()\n",
        "        else:\n",
        "            self.parse_test_annotations_file()\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        \n",
        "        img = Image.open(self.img_info[index][\"img_path\"]).convert(\"RGB\")\n",
        "        target = None if self.split == \"test\" else self.img_info[index][\"annotations\"]\n",
        "        if self.transform is not None:\n",
        "            img, target = self.transform(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_info)\n",
        "    \n",
        "    def extra_repr(self) -> str:\n",
        "        lines = [\"Split: {split}\"]\n",
        "        return \"\\n\".join(lines).format(**self.__dict__)\n",
        "\n",
        "    def parse_train_val_annotations_file(self) -> None:\n",
        "        filename = \"wider_face_train_bbx_gt.txt\" if self.split == \"train\" else \"wider_face_val_bbx_gt.txt\"\n",
        "        filepath = os.path.join(self.root, \"wider_face_split\", filename)\n",
        "\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "            file_name_line, num_boxes_line, box_annotation_line = True, False, False\n",
        "            num_boxes, box_counter, k = 0, 0, 0\n",
        "            labels = []\n",
        "            for line in lines:\n",
        "                line = line.rstrip()\n",
        "                if file_name_line: \n",
        "                    img_path = os.path.join(self.root, \"WIDER_\" + self.split, \"images\", line)\n",
        "                    img_path = abspath(expanduser(img_path))\n",
        "                    file_name_line = False\n",
        "                    num_boxes_line = True\n",
        "                elif num_boxes_line:\n",
        "                    num_boxes = int(line)\n",
        "                    num_boxes_line = False\n",
        "                    box_annotation_line = True\n",
        "                elif box_annotation_line:\n",
        "                    box_counter += 1\n",
        "                    line_split = line.split(\" \")\n",
        "                    line_values = [int(x) for x in line_split]\n",
        "                    line_values = np.array(line_values, dtype=np.float32)\n",
        "\n",
        "                    xmin = line_values[0].copy()\n",
        "                    xmax = line_values[2].copy()\n",
        "                    ymin = line_values[1].copy()\n",
        "                    ymax = line_values[3].copy()\n",
        "\n",
        "                    if xmin > xmax:\n",
        "                        xmin, xmax = xmax, xmin\n",
        "                    elif xmin == xmax:\n",
        "                        xmax += 1\n",
        "\n",
        "                    if ymin > ymax:\n",
        "                        ymin, ymax = ymax, ymin\n",
        "                    elif ymin == xmax:\n",
        "                        ymax += 1\n",
        "\n",
        "                    line_values = []\n",
        "                    line_values = np.array(line_values, dtype=np.float32)\n",
        "                    line_values = np.append(line_values, xmin)\n",
        "                    line_values = np.append(line_values, ymin)\n",
        "                    line_values = np.append(line_values, xmax)\n",
        "                    line_values = np.append(line_values, ymax)\n",
        "                    if xmin < xmax and ymin < ymax and ((xmax - xmin)*(ymax-ymin)) > 0:\n",
        "                        labels.append(line_values)\n",
        "                    else:\n",
        "                        k += 1\n",
        "                    \n",
        "                    if box_counter >= num_boxes:\n",
        "                        box_annotation_line = False\n",
        "                        file_name_line = True\n",
        "                        if num_boxes-k > 0:\n",
        "                            labels_tensor = torch.tensor(labels)\n",
        "                            area = (labels_tensor[:, 3] - labels_tensor[:, 1])*(labels_tensor[:, 2] - labels_tensor[:, 0])\n",
        "                            self.img_info.append(\n",
        "                                { \n",
        "                                    \"img_path\": img_path,\n",
        "                                    \"annotations\": {\n",
        "                                        \"boxes\": labels_tensor[:, 0:4],\n",
        "                                        \"labels\": torch.ones((box_counter-k,), dtype=torch.int64),\n",
        "                                        \"image_id\": torch.ones(1),\n",
        "                                        \"area\": area,\n",
        "                                        \"iscrowd\": torch.zeros((box_counter-k,), dtype=torch.int64)\n",
        "                                    }\n",
        "                                }\n",
        "                            )\n",
        "                        box_counter, k = 0, 0\n",
        "                        labels.clear()\n",
        "                else:\n",
        "                    raise RuntimeError(f\"Error parsing annotation file {filepath}\")\n",
        "\n",
        "    def parse_test_annotations_file(self) -> None:\n",
        "        filepath = os.path.join(self.root, \"wider_face_split\", \"wider_face_test_filelist.txt\")\n",
        "        filepath = abspath(expanduser(filepath))\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                line = line.rstrip()\n",
        "                img_path = os.path.join(self.root, \"WISER_test\", \"images\", line)\n",
        "                img_path = abspath(expanduser(img_path))\n",
        "                self.img_info.append({\"img_path\": img_path})\n",
        "\n",
        "    def _check_integrity(self) -> bool:\n",
        "        # Allow original archive to be deleted (zip). Only need the extracted images\n",
        "        all_files = self.FILE_LIST.copy()\n",
        "        all_files.append(self.ANNOTATIONS_FILE)\n",
        "        for (_, md5, filename) in all_files:\n",
        "            file, ext = os.path.splitext(filename)\n",
        "            extracted_dir = os.path.join(self.root, file)\n",
        "            if not os.path.exists(extracted_dir):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def download(self) -> None:\n",
        "        if self._check_integrity():\n",
        "            print(\"Files already downloaded and verified\")\n",
        "            return\n",
        "\n",
        "        # download and extract image data\n",
        "        for (file_id, md5, filename) in self.FILE_LIST:\n",
        "            download_file_from_google_drive(file_id, self.root, filename, md5)\n",
        "            filepath = os.path.join(self.root, filename)\n",
        "            extract_archive(filepath)\n",
        "\n",
        "        # download and extract annotation files\n",
        "        download_and_extract_archive(\n",
        "            url=self.ANNOTATIONS_FILE[0], download_root=self.root, md5=self.ANNOTATIONS_FILE[1]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "Lhp_WomDnKA6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
        "import torch\n",
        "\n",
        "fasterRCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "# Define RPN \n",
        "anchor_generator = AnchorGenerator(sizes=tuple([(16, 32, 64, 128, 256) for _ in range(5)]), # let num of tuple equal to num of feature maps\n",
        "                                  aspect_ratios=tuple([(0.75, 0.5, 1.25) for _ in range(5)])) # ref: https://github.com/pytorch/vision/issues/978\n",
        "rpn_head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
        "fasterRCNN.rpn = RegionProposalNetwork(\n",
        "    anchor_generator= anchor_generator, head= rpn_head,\n",
        "    fg_iou_thresh= 0.7, bg_iou_thresh=0.3,\n",
        "    batch_size_per_image=48, # use fewer proposals\n",
        "    positive_fraction = 0.5,\n",
        "    pre_nms_top_n=dict(training=200, testing=100),\n",
        "    post_nms_top_n=dict(training=160, testing=80),\n",
        "    nms_thresh = 0.7\n",
        ")"
      ],
      "metadata": {
        "id": "K6ztowQhVeAs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = fasterRCNN.roi_heads.box_predictor.cls_score.in_features #get number of features\n",
        "fasterRCNN.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = 2)\n",
        "fasterRCNN.roi_heads.fg_bg_sampler.batch_size_per_image = 24\n",
        "fasterRCNN.roi_heads.fg_bg_sampler.positive_fraction = 0.5"
      ],
      "metadata": {
        "id": "_3uhllReV2Vq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = WiderDataset('WIDER', 'train',  get_transform(train=True))\n",
        "dataset_test = WiderDataset('WIDER', 'val',  get_transform(train=True))\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "metadata": {
        "id": "jKbqpWL3WMEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9ff5f1-c689-4a25-ba8d-98e4dc1586a2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move model to the right device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "fasterRCNN.to(device)\n",
        "\n",
        "params = [p for p in fasterRCNN.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(params, lr=0.0005, betas=(0.9, 0.999), weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "metric_collector = []\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 5 iterations\n",
        "    metric_logger = train_one_epoch(fasterRCNN, optimizer, data_loader, device, epoch, print_freq=5)\n",
        "    metric_collector.append(metric_logger)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # Evaluate with validation dataset\n",
        "    metric_logger_val = validate(fasterRCNN, data_loader_test, device, print_freq=5)\n",
        "    #save checlpoint\n",
        "    torch.save( fasterRCNN.state_dict(), os.path.join( weights_path,'fasterRCNN_ep'+str(epoch)+'.pth') )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "WfFw-RMUV_2J",
        "outputId": "504705cc-2413-49a1-b928-8c9d32a53ce1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [   0/6425]  eta: 4:00:24  lr: 0.000001  loss: 2.6776 (2.6776)  loss_classifier: 0.8727 (0.8727)  loss_box_reg: 0.0806 (0.0806)  loss_objectness: 0.6794 (0.6794)  loss_rpn_box_reg: 1.0449 (1.0449)  time: 2.2451  data: 0.7530  max mem: 10805\n",
            "Epoch: [0]  [   5/6425]  eta: 2:57:17  lr: 0.000003  loss: 2.5917 (2.4297)  loss_classifier: 0.8664 (0.8491)  loss_box_reg: 0.1662 (0.2299)  loss_objectness: 0.6794 (0.6791)  loss_rpn_box_reg: 0.7693 (0.6716)  time: 1.6570  data: 0.1347  max mem: 10805\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1d58c5723a5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# train for one epoch, printing every 5 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmetric_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasterRCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmetric_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_gt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_targets_to_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_gt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             loss_objectness, loss_rpn_box_reg = self.compute_loss(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36massign_targets_to_anchors\u001b[0;34m(self, anchors, targets)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mlabels_per_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mmatch_quality_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors_per_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_quality_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# get the targets corresponding GT for each proposal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mbox_iou\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mNxM\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpairwise\u001b[0m \u001b[0mIoU\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevery\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboxes1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \"\"\"\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0minter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_box_inter_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36m_box_inter_union\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mrb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0mwh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 0; 11.17 GiB total capacity; 9.40 GiB already allocated; 242.81 MiB free; 10.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ]
}